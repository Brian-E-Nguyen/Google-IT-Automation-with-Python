{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating Cloud Deployments\n",
    "\n",
    "## 1. Cloud Scale Deployments\n",
    "\n",
    "Over the past few videos, we've checked out some of the features we can use when running services in the Cloud. As we called out before, the biggest advantage of using Cloud services is how easily we can scale our services up and down. Now, to make the most out of this advantage, we need to do some preparation. We'll set up our services so that we can easily increase their capacity by adding more nodes to the pool. These nodes could be virtual machines, containers, or even specific applications providing one service. \n",
    "\n",
    "### 1.1 Load Balancers\n",
    "\n",
    "Whenever we have a service with a bunch of different instances serving the same purpose, we'll use a load balancer. **A load balancer** *ensures that each node receives a balanced number of requests.* When a request comes in, the load balancer picks a node to serve the response. There's a bunch of different strategies load balancer uses to select the node. The simplest one is just to give each node one request called round robin. More complex strategies include always selecting the same node for requests coming from the same origin, selecting the node that's closest to the requester, and selecting the one with the least current load. \n",
    "\n",
    "### 1.2 Autoscaling\n",
    "\n",
    "As we called out, instance groups like these are usually configured to spin up more nodes when there's more demand, and to shut some nodes down when the demand falls. This capability is called **autoscaling.** *It allows the service to increase or reduce capacity as needed while the service owner only pays for the cost of the machines that are in use at any given time.* Since some nodes will shut down when demand is lower, their local disks will also disappear and should be considered ephemeral or short-lived. If you need data persistence, you'll have to create separate storage resources to hold that data and connect that storage to the nodes. That's why the services that we run in the Cloud are usually connected to a database which is also running in the Cloud. This database will also be served by multiple nodes behind a load balancer, but this is typically managed by the Cloud provider using the platform as a service model. \n",
    "\n",
    "### 1.3 How These Work\n",
    "\n",
    "To check out how this works in practice, let's look at an example of a web application with a lot of users. When you connect to a site through the Internet, your web browser first retrieves an IP address for the website that you want to visit. This IP address identifies a specific computer, the entry point for the sites. Commonly there will be a bunch of different entry points for a single website. This allows the service to stay up even if one of them fails. \n",
    "\n",
    "![img45](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-3-Automation-in-the-Cloud/img/img45.jpg?raw=true)\n",
    "\n",
    "On top of that, it's possible to select an entry point that's closer to the user to reduce latency. In a small-scale application, this entry point could be the web server that serves the pages, and that would be it. For large applications where speed and availability matter, there will be a couple of layers in between the entry point and the actual web service. \n",
    "\n",
    "![img46](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-3-Automation-in-the-Cloud/img/img46.jpg?raw=true)\n",
    "\n",
    "The first layer will be a pool of web caching servers with a load balancer to distribute the requests among them. One of the most popular applications for this caching is called Varnish, but of course it's not the only one. The Nginx web server and software also includes this caching functionality. There's a bunch of providers that do web caching as a service like Cloudflare and Fastly. No matter the software used, the result is basically the same. When a request is made, the caching servers first check if the content is already stored in their memory. If it's there, they respond with the contents, if it's not, they ask their configured backend for the content and then store it so that it's present for future requests. \n",
    "\n",
    "![img47](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-3-Automation-in-the-Cloud/img/img47.jpg?raw=true)\n",
    "\n",
    "This configured backend is the actual web service that generates the webpages for the site, and it will also normally be a pool of nodes running under a load balancer. To get any necessary data, this service will connect to a database. But because getting data from a database can be slow, there's usually an extra layer of caching, specific for the database contents. \n",
    "\n",
    "The most popular applications for this level of caching are Memcached and Redis. As you can see, there is a lot of different nodes in this scheme. Fortunately, once you've done your homework and prepared your setup, you can rely on the capabilities offered by the Cloud provider to automatically scale the system up and down as necessary. The infrastructure will take care of adding and removing instances, distributing the load, making sure that each geographical region has the right capacity, and a bunch more things. Up next, we'll talk about some of the things we can do when we need to have more control over the instances that are running our service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Orchestration?\n",
    "\n",
    "Throughout this course and the entire program, we've been talking about automation. As a reminder, automation is the process of replacing a manual step with one that happens automatically. In the past few videos, we've mentioned a few ways that let us automate the creation of Cloud instances. We can use templating to create new virtual machines, we can run a command line tool that automatically creates new instances for us, or we can choose to enable auto-scaling and let the infrastructure tools take care of that depending on the demand. But all of this automatic creation of new instances needs to be coordinated so that the instances correctly interact with each other and that's where orchestration comes into play. \n",
    "\n",
    "**Orchestration** *is the automated configuration and coordination of complex IT systems and services.* In other words, orchestration means automating a lot of different things that need to talk to each other. This will always include a lot of different automated tasks and will generally involve configuring a bunch of different systems. Taking the example of the website infrastructure that we saw in our last video, we've seen how we can automate the creation of each instance in the system. Now, say you wanted to deploy a new copy of the system in a separate data center where you have no instances yet, you'll need to also automate the whole configuration of the system, the different instance types involved, how will each instance finesse the others, what the internal network looks like, and so on. \n",
    "\n",
    "### 2.1 How Does This Work?\n",
    "\n",
    "So how does this work? The key here is that the configuration of the overall system needs to be automatically repeatable. There's a bunch of different tools that we can use to do that. These tools typically don't communicate with the Cloud systems through the web interface or the command line. They normally use an **application programming interface or API** that lets us interact with the Cloud infrastructure directly from our scripts. We'll talk more about other APIs in a later course. \n",
    "\n",
    "In the case of Cloud provider APIs, they typically let you handle the configuration that you want to sit directly from your scripts or programs without having to call a separate command. This combines the power of programming with all of the available Cloud resources. The APIs offered by the Cloud providers let us perform all the tasks that we mentioned earlier like creating, modifying, and deleting instances and also deploying complex configurations for how these instances will talk to each other. All of these actions can also be completed through the web interface or the command line. But doing them from our programs gives us extra flexibility which can be key when automating complex setups.\n",
    "\n",
    "Say you wanted to deploy a system that combines some services running on a Cloud provider and some services running on-premise, this is known as a **hybrid Cloud** setup, or only part of the services are in the Cloud. The setup is super common in the industry right now. Orchestration tools can be a pretty useful tool to make sure that both the on-premise services and the Cloud services know how to talk to each other and are configured with the right settings. \n",
    "\n",
    "![img48](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-3-Automation-in-the-Cloud/img/img48.jpg?raw=true)\n",
    "\n",
    "Going back to the website example that we discussed earlier to make sure that the service is running smoothly, we should set up a monitoring and alerting. This lets us detect and correct any problems with our service before users even notice. This is a critical piece of infrastructure but setting it up correctly can take quite some time. \n",
    "\n",
    "![img49](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-3-Automation-in-the-Cloud/img/img49.jpg?raw=true)\n",
    "\n",
    "By using orchestration tools, we can automate the configuration of any monitoring rules that we need to set, which metrics we want to look for, when we want to be alerted, and so on, and automatically apply these to a complete deployment no matter which datacenter the services are running in. This might seem like a super complex task, but fortunately there are tools available to make our lives easier. We'll talk about those in our next video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
