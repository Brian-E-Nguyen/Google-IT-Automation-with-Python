{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring and Alerting\n",
    "\n",
    "## 1. Getting Started with Monitoring\n",
    "\n",
    "### 1.1 Basic Info\n",
    "\n",
    "As we called out in an earlier video, once we have our service running in the Cloud, we want to make sure that our service keeps running, and not just that, we want to make sure it keeps behaving as expected, returning the right results quickly and reliably. The key to ensuring all of this, is to set up good monitoring and alerting rules. In the next few videos, we'll do a rundown of monitoring and alerting concepts and techniques, followed by a practical demonstration. Let's dive in. \n",
    "\n",
    "To understand how our service is performing, we need to monitor it. **Monitoring** *lets us look into the history and current status of a system.* How can we know what the status is? We'll check out a bunch of different metrics. These **metrics** *tell us if the service is behaving as expected or not.* Well, some metrics are generic, like how much memory an instance is using. Other metrics are specific to the service we want to monitor. \n",
    "\n",
    "### 1.2 Example\n",
    "\n",
    "Say your company is running a website and you want to check if it's working correctly. When a web server responds to an HTTP request, it starts by sending a response code, followed by the content of the response. You might know, for example, that a 404 code means that the page wasn't found, or that a 500 response means that there was an internal server error. \n",
    "\n",
    "![img16](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img16.jpg?raw=true)\n",
    "\n",
    "In general, response codes in the 500 range, like 501 or 503, tells us that something bad happened on the server while generating a response. Well, response codes in the 400 range means there was a client-side problem in the request. When monitoring your web service, you want to check both the count of response codes and their types to know if everything's okay. If you're running an e-commerce site, you'll care about how many purchases were made successfully and how many failed to complete. If you're running a mail server, you want to know how many emails were sent and how many got stuck and so on. You'll need to think about the service you want to monitor and figure out the metrics you'll need. \n",
    "\n",
    "### 1.3 What to Do With Metrics\n",
    "\n",
    "Now, once we've decided what metrics we care about, what do we do with them? We'll typically store them in the monitoring system. There's a bunch of different monitoring systems out there. Some systems like AWS Cloudwatch, Google Stack Driver, or Azure Metrics are offered directly by the Cloud providers. Other systems like Prometheus, Datadog, or Nagios can be used across vendors. There's two ways of getting our metrics into the monitoring system. Some systems use a pull model, which means that the monitoring infrastructure periodically queries our service to get the metrics. Other monitoring systems use a push model, which means that our service needs to periodically connect to the system to send the metrics. \n",
    "\n",
    "![img17](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img17.jpg?raw=true)\n",
    "\n",
    "No matter how we get the metrics into the system, we can create dashboards based on the collected data. This dashboard show the progression of the metrics over time. We can look at the history of one specific metric to compare the current state to how it was last week or last month. Or we can look at the progression of two or more metrics together to check out how the change in one metrics effects another. Imagine it's Monday morning and you notice that your service is receiving a lot less traffic than usual. You can look at the data from past weeks and see if you always get less traffic on Monday mornings or if there's something broken causing your service to be unresponsive. Or if you see that in the past couple days, the memory used by your instances has been going up, you can check if this growth follows a similar increase in another metric, like the amount of requests received or the amount of data being transmitted. This can help you decide if there's been a memory leak that needs to be fixed or if it's just an expected consequences of a growth in popularity.\n",
    "\n",
    "Pro tip, **you only want to store the metrics that you care about, since storing all of these metrics in the system takes space, and storage space costs money.**\n",
    "\n",
    "![img18](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img18.jpg?raw=true)\n",
    "\n",
    "### 1.4 Whitebox and Blackbox Monitoring\n",
    "\n",
    "When we collect metrics from inside a system, like how much storage space the service is currently using or how long it takes to process a request, this is called whitebox monitoring. **Whitebox monitoring** *checks the behavior of the system from the inside.* We know the information we want to track, and we're in charge of making it possible to track. For example, if we want to track how many queries we're making to the database, we might need to add a variable to count this. \n",
    "\n",
    "On the flip side, **blackbox monitoring** *checks the behavior of the system from the outside.* This is typically done by making a request to the service and then checking that the actual response matches the expected response. We can use this to do a very simple check to know if the service is up and to verify if the service is responding from outside your network. Or we could use it to see how long it takes for a client in a different part of the world to get a response from the system. \n",
    "\n",
    "Okay, monitoring is really cool, but who wants to stare at dashboards all day trying to figure out if something's wrong? Fortunately, we don't have to. Instead, we can set up alerting rules to let us know if something's wrong. This is a critical part of ensuring a reliable system, and we're going to learn how to do it in the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting Alerts When Things Go Wrong\n",
    "\n",
    "We expect a lot from our modern IT services. We expect them to be up and running 24-7. We want to be able to get our work done whenever and wherever. For that, we need our services to respond day or night, workday or holiday. But even if the services are running 24-7, System Administrators can't constantly be in front of their systems. Instead, we set up our services so that they work unattended and deal with problems when they happen. \n",
    "\n",
    "### 2.1 Simple Alert Example\n",
    "\n",
    "Now to do this, we need to detect those problems so that we can deal with them as quickly as possible. If you have no automated way of raising an alert, you might only find out about the issue when you get a call from a frustrated user telling you that your service is down. That's not ideal. It's much better to create automation that checks the health of your system and notifies you when things don't behave as expected. This can give you advance warning that something's wrong, sometimes even before users notice a problem at all. \n",
    "\n",
    "So how do we do that? The most basic approach is to run a job periodically that checks the health of the system and sends out an email if the system isn't healthy. On a Linux system, we could do this using **cron,** which is the tool to schedule periodic jobs. We'd pair this with a simple Python script that checks the service and sends any necessary emails. This is an extremely simplified version of an alerting system, but it shares the same principles. Is all alerting systems, no matter how complex and advanced. We want to periodically check the state of the service and raise alerts if there's a problem. \n",
    "\n",
    "When you use a monitoring system like the ones we described in our last video, the metrics you collect represent the state of your service. Instead of periodically running a script that connects to the service and checks if it's responding, you can configure the system to periodically evaluate the metrics; and based on some conditions, decide if an alert should be raised. \n",
    "\n",
    "**Raising an alert signals that something is broken and a human needs to respond.** For example, you can set up your system to raise alerts if the application is using more than 10 gigabytes of RAM, or if it's responding with too many 500 errors, or if the queue of requests waiting to get processed gets too long. \n",
    "\n",
    "### 2.2 Nonurgent Alerts\n",
    "\n",
    "Of course, not all alerts are equally urgent. We typically divide useful alerts into two groups, those that need immediate attention and those that need attention in the near future. If an alert doesn't need attention, then it shouldn't have been sent at all. It's just noise. If your web service is responding with errors to 50 percent of the requests, you should look at what's going on right away. Even if this means waking up in the middle of the night to address whatever is wrong, you'll definitely want to fix this kind of critical problem ASAP. \n",
    "\n",
    "On the other hand, if the issue is that the attached storage is 80 percent full, you need to figure out whether to increase the disk size or maybe clean up some of the stored data. But this isn't super urgent, so don't let it get in the way of a good night's sleep. Since these two types of alerts are different, we typically configure our systems to raise alerts in two different ways. Those that need immediate attention are called **pages,** which comes from a device called a pager. Before mobile phones became popular, pagers were the device of choice for receiving urgent messages, and they're still used in some places around the world. Nowadays, most people receive their pages in other forms like SMS, automated phone calls, emails, or through a mobile app, but we still call them pages. \n",
    "\n",
    "On the flip side, the non-urgent alerts are usually configured to create bugs or tickets for an IT specialist to take care of during their workday. They can also be configured to send email to specific mailing lists or send a message to a chat channel that will be seen by the people maintaining the service. \n",
    "\n",
    "One thing to highlight is that all alerts should be **actionable.** If you get a bug or a page and there's nothing for you to do, then the alert isn't actionable and it should be changed or it shouldn't be there at all. Otherwise, it's just **noise.** Say you're trying to check if your services database back-end is responsive. If you do this by creating a query that returns all rows in a large table, your request might sometimes timeout and raise an alert. That would be a noisy alert, not really actionable. You'd need to tweak the query to make the check useful. \n",
    "\n",
    "![img19](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img19.jpg?raw=true)\n",
    "\n",
    "Say you run a cron job that copies files from one location to another every 10 minutes, you want to check that this job runs successfully. So you configure your system to alert you if the job fails. After putting this in production, you realize there's a bunch of unimportant reasons that can cause this job to temporarily fail. \n",
    "\n",
    "![img20](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img20.jpg?raw=true)\n",
    "\n",
    "Maybe the destination storage is too busy and so sometimes the job times out. Maybe the origin was being rebooted right when the job started, so the job couldn't connect to it. No matter why, whenever you go to check out what caused a job to fail, you discover that the following run had succeeded and there's nothing for you to do. \n",
    "\n",
    "You need to rethink the problem and tweak your alert. Since the task is running frequently, you don't care if it fails once or twice, you can change the system to only raise the alert if the job fails three times in a row. That way when you get a bug, it means that it's failing consistently and you'll actually need to take action to fix it. \n",
    "\n",
    "All of this configuring and tweaking can seem like a lot of work. You need to think about which metrics you care about. Configure your monitoring system to store them, then configure your alerting system to raise alerts when things don't behave as expected. The flip side is that once you've set your systems to raise actionable alerts when needed, you're going to have peace of mind. If no alerts are firing, you know the service is working fine. This lets you concentrate on other tasks without having to worry. To set up good alerts, we need to figure out which situations should page, which ones should create bugs, and which ones we just don't care about. These decisions aren't always easy and might need some discussion with the rest of your team. But it can help make sure that you spend time only on things that actually matter. \n",
    "\n",
    "Up next, we'll talk about criteria that we can use to decide which situation should raise alerts and what to do about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Service-Level Objectives\n",
    "\n",
    "\n",
    "### 3.1 Basic Info\n",
    "\n",
    "We all know that some IT systems are more critical than others. Let's be real, if you try to play a computer game that you haven't opened in a year and it doesn't work, you probably won't care as much as if you're trying to make a bank transfer and your bank's website is down. Sometimes a piece of infrastructure can be down and the overall system still works with degraded performance. For example, if the caching server that makes your web application go faster is down, the app can still function, even if it's running slower. No system is ever available 100% of the time, it's just not possible. But depending on how critical the service is, it can have different **service level objectives, or SLOs.** SLOs *are pre-established performance goals for a specific service.* Setting these objectives helps manage the expectations of the service users, and the targets also guide the work of those responsible for keeping the service running. SLOs need to be **measurable,** *which means that there should be metrics that track how the service is performing and let you check if it's meeting the objectives or not.* Many SLOs are expressed as how much time a service will behave as expected. For example, a service might promise to be available 99% of the time.\n",
    "\n",
    "### 3.2 Dealing w/ Metrics & Availability\n",
    "\n",
    "Heads up, when dealing with metrics and availability, we need to do a little math to understand what those numbers mean in practice, but don't worry, it's all pretty straightforward. If our service has an SLO of 99% availability, it means it can be down up to 1 % of the time. If we measure this over a year, the service can be down for a total of 3.65 during the year and still have 99% availability. Availability targets like this one are commonly named by their number of nines. Our 99% example would be a two 9 service, 99.9% availability is a three 9 service, 99.999% availability is a five 9 service. Five nine services promised a total down time of up to five minutes in a year. Five nines is super high availability, reserved only for the most critical systems. A three nine service, aiming for a maximum of eight hours of downtime per year, is fine for a lot of IT systems. \n",
    "\n",
    "Now, you might be wondering, why not just make everything a five nine service? It's a good question. The answer is, because it's really expensive and usually not necessary. If your service isn't super critical and it's okay for it to be down briefly once in a while having two or three nines of availability might be enough. You can keep the service running with a small team. Five nine services usually require a much larger team of engineers to maintain it. Any service can have a bunch of different service level objectives like these, they tell its users what to expect from it. \n",
    "\n",
    "### 3.3 Service Level Agreements (SLA's)\n",
    "\n",
    "Some services, like those that we pay for, also have more strict promises in the form of service level agreements, or SLAs. A **service level agreement** *is a commitment between a provider and a client.* Breaking these promises might have serious consequences. Service level objectives though are more like a soft target, it's what the maintaining team aims for, but the target might be missed in some situations. \n",
    "\n",
    "### 3.4 SLO's and SLA's Together\n",
    "\n",
    "As we called out, having explicit SLOs or SLAs is useful for both the users of that service and the team that keeps the service running. If you're using a cloud service, you can decide how much you're going to entrust your infrastructure to it, based on the SLAs that the provider publishes. If on the other hand you're part of the team that maintains the service, you can use the SLOs and SLAs of your service to decide which alerts to create and how urgent they should be. \n",
    "\n",
    "Say you have a service with an SLO that says that at least 90% of the requests should return within 5 seconds. To know if your service is behaving correctly, you need to measure how many of the total requests are returning within those 5 seconds, and you want that number to always be above 90%. \n",
    "\n",
    "![img21](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img21.jpg?raw=true)\n",
    "\n",
    "So you might set up a non-paging alert to notify you if less than 95% return within 5 seconds, and a paging alert if less than 90% return promptly. If you're in charge of a website, you'll typically measure the rate of responses with 500 return codes to check if your service is behaving correctly. If your SLO is 99% of successful requests, you can set up a non-paging alert if the rate of errors is above 0.5%, and a paging alert if it reaches 1%. \n",
    "\n",
    "### 3.5 Services That Are Breaking\n",
    "\n",
    "In an earlier video, we called out that services usually break because something changed. That's also often the case when looking at what makes services go out of SLO. If your service was working fine and meeting all of its SLOs and then started misbehaving, it's likely this was caused by a recent change. That's why some teams use the concepts of error budgets to handle their services.\n",
    "\n",
    "Say you're running a service that has three nines of availability. This means the service can be down 43 minutes per month, this is your error budget. You can tolerate up to 43 minutes of downtime, so you keep track of the total time the service was down during the month. If it starts to get close to those 43 minutes, you might decide to stop pushing any new features and focus on resolving the problems that keep causing the downtime. \n",
    "\n",
    "![img22](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img22.jpg?raw=true)\n",
    "\n",
    "Now, all this talk of nines, availability and downtime can have your head spinning if you've never done this before, and that's totally normal. If it's your first time setting objectives for your service, start by setting achievable goals that you can measure. Track how the service behaves for a while and see what causes the service to deviate from the targets. Once you have a better idea of the whole service's behavior, you can set more aggressive goals. Up next, we'll go back to our VMS running in the cloud and demonstrate how we can monitor them using the tools offered by the provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Monitoring in GCP\n",
    "\n",
    "So far, we've seen how to create virtual machines in the Google Cloud Console. We've kept these virtual machines running and now we want to see how we can use the tools provided by the cloud vendor to monitor them and create alerts based on them. For this demonstration, we'll use the monitoring tool called Stackdriver, which is part of the overall offering. When you first activate this system, it takes a while until it starts collecting on the metrics from all the machines, so we've activated in advance.\n",
    "\n",
    "![img23](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img23.jpg?raw=true)\n",
    "\n",
    "When we first opened the monitoring console, we see an overview of the system. At the moment, this is looking pretty empty, but we could configure this dashboard to show the charts that we consider the most useful. Let's go into the instances dashboard,\n",
    "\n",
    "![img24](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img24.jpg?raw=true)\n",
    "\n",
    "we see here the list of our instances and we can click on each of them to see that monitoring information that Stackdriver has collected about them. \n",
    "\n",
    "![img25](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img25.jpg?raw=true)\n",
    "\n",
    "### 4.1 Setting Up an Alert\n",
    "\n",
    "The monitoring system gives us a very simple overview of each of the instances with three basic metrics, CPU usage, Disk I/O, and network traffic. Depending on what surface we want to run on a VM, we can customize these dashboards to show different metrics. If the metrics that come baked in aren't enough, you can create your own metrics and also add them here. Now we want to check out how to set up an alert to notify us if something isn't behaving correctly. To do this, we'll create a new alerting policy. \n",
    "\n",
    "![img26](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img26.jpg?raw=true)\n",
    "\n",
    "![img27](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img27.jpg?raw=true)\n",
    "\n",
    "To set up a new alert, we have to configure the condition that triggers the alert. After we've done that, we can also configure how we want to be notified of the issue and add any documentation that we want the notification to include. Let's start by configuring the condition. \n",
    "\n",
    "![img28](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img28.jpg?raw=true)\n",
    "\n",
    "### 4.2 Selecting the Metrics\n",
    "\n",
    "As we called out, alerting conditions are related to specific metrics. We want to be notified when the metric indicates that there's a problem with an instance. For this example, we're going to configure an alert that triggers if an instance in CPU utilization is more than 90 percent. We'll start by selecting that we want to monitor GCE, VM instances, which are the instances that we currently have running and then select the CPU utilization metric.\n",
    "\n",
    "![img29](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img29.jpg?raw=true)\n",
    "\n",
    "![img30](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img30.jpg?raw=true)\n",
    "\n",
    "After selecting the metric, we see the graph of the collected values for all of the current running instances. We can optionally add extra filters and groups for the data for this condition. For example, we could choose to only look at some of the instances, selecting by their zone, region, or name. This can be useful if you want to have separate alerts for instances used for production, and those used for testing or development. On top of that, we can also choose an aggregator for the data, these aggregators are useful when the metrics that you're collecting are about the overall system and not just one instance. \n",
    "\n",
    "![img31](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img31.jpg?raw=true)\n",
    "\n",
    "For example, if you're checking the number of error responses that your system generated, you want to sum all the errors across instances. Depending on how we filter group and aggregate the data, we'll end up with a bunch of different time series, we'll use these values to decide if we should trigger the alert or not. \n",
    "\n",
    "### 4.3 Frequency of Alerts\n",
    "\n",
    "The next step is selecting how many of the different time series need to violate the condition for the alert to trigger. We can trigger the alert when one, some, or all of the different time series violate the condition.\n",
    "\n",
    "![img32](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img32.jpg?raw=true)\n",
    "\n",
    "For this example, we'll configure our alert to trigger if any instance is using more than 90 percent of the CPU. So, let's select any time series violates. Now, we'll say that we want our alert to trigger if the value is above 90 percent for one minute. \n",
    "\n",
    "### 4.4 How to Receive Notifications\n",
    "\n",
    "All right. We've set the condition. Now, we can select how we want to get the notification and when the alert triggers. Currently, the only type of notification that we can use is e-mail. To use the other channel types available, we need to configure them in our profile. For this example, e-mail will do. Using e-mails can be just fine when you're getting started with alerting, but eventually you'll want to configure additional methods. \n",
    "\n",
    "![img33](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img33.jpg?raw=true)\n",
    "\n",
    "All right. We've configured our alert to send e-mails. Now, we can add extra documentation to our alert. This documentation is intended to help the person that's responding to the alert understand what they need to do to fix the problem. Including good documentation here, it can be super-important when you've got a bunch of different people working together in a team and not everyone knows everything. Alerts that include good documentation are much easier to tend to and help get the service back to a healthy state faster. For our example, we'll add a message saying that whoever is taking care of this alert, should check the instance with top. Finally, we'll need to give a name to our alerting policy, we'll call it CPU and then save it.\n",
    "\n",
    "![img34](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img34.jpg?raw=true)\n",
    "\n",
    "![img35](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img35.jpg?raw=true)\n",
    "\n",
    "### 4.5 What Happens When the Alert Triggers\n",
    "\n",
    "Now, we've set up our alert. Now, we can sit back and relax knowing that if anything goes wrong, we'll be the first to know. For the final part of this demo, we want to show what happens when the alert triggers. To do that, we'll start a process in one of our instances that uses all of the available CPU, by creating an infinite loop. So we'll go back to the main console, SSH into the VM, called Linux- instance and then create a wire loop that never ends.\n",
    "\n",
    "![img36](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img36.jpg?raw=true)\n",
    "\n",
    "Now, our loop is running and using all of the available CPU, we can check this by running the top command that shows us the CPU usage. \n",
    "\n",
    "![img37](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img37.jpg?raw=true)\n",
    "\n",
    "We see that there's a bash command that's using almost 100 percent of the available CPU, our experiment is working. Now, remember that we said that we wanted the condition to be true for a minute before the alert triggers, it won't trigger just yet. It's common practice to use time windows of one, five, or even 10 minutes when dealing with the alerting. We don't want to get an alert for a small spike that lasted only a few seconds and then went away. We want to get alerted when there's an actual problem that requires our attention. The size of the time window we choose depends on the metric we're checking, the length of the expected spikes and a bunch of other factors. It's pretty normal to have to tweak how long we want the condition to be true as we try our alert out. If you're getting notified too often about conditions that go away on their own without you having to do anything, you might choose to make the time window larger. On the flip side, if you're getting notified too late about conditions that needed attention, you might choose to make the time window smaller. All right. We've let enough time pass, let's check out what's up with our alert.\n",
    "\n",
    "![img38](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img38.jpg?raw=true)\n",
    "\n",
    "We see that there's an open incident, which is a way of grouping problems. The alerts summary gives us a bunch of info about what's going on. We can click on the CPU link to get more information. \n",
    "\n",
    "![img39](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img39.jpg?raw=true)\n",
    "\n",
    "This page shows us the metric that triggered the alert for the incident, it shows the threshold for triggering the alert and the current value of the metric. It also shows us the documentation that we entered and lets us create annotations. We can use these annotations to track the work that we do during an incident. All right. Let's stop the process that's using all of our instances CPU. It's still running the top process from before. Let's exit with Q. Now, the infinite loop is currently running in the background of our console. We can make it run in the foreground by typing fg, and then cancel it by pressing CTRL+C. \n",
    "\n",
    "![img40](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img40.jpg?raw=true)\n",
    "\n",
    "Now, we've stopped the process. Let's check with top that it's no longer using all of our CPU. Great, the bash process isn't taking all of the CPU time anymore. In another minute, the alert that we'd saw earlier will stop triggering, nice. \n",
    "\n",
    "![img41](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img41.jpg?raw=true)\n",
    "\n",
    "![img42](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img42.jpg?raw=true)\n",
    "\n",
    "With that, we've demonstrated how we can monitor a bunch of instances running in the Cloud. We've created an alert based on metrics and verify that the alert triggers. Of course, there's a lot more that we can do with these tools, we'll give you pointers to more information in the reading coming up. After that, there's another quick quiz to check that everything is making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
