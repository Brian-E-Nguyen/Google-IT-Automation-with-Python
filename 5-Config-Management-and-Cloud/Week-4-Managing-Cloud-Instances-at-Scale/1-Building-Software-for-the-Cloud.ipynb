{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Software for the Cloud\n",
    "\n",
    "## 1. Intro to Module 4: Managing Cloud Instances at Scale\n",
    "\n",
    "Welcome back. And guess what? This is the last module in the course. Congratulations on making it here. It's sure been an exciting ride and it's about to get even more interesting. \n",
    "\n",
    "As we've learned in the past modules Cloud providers offer us a bunch of different services. By now we've learned a lot about different hosting models. If you're working for a small company you might get by with pre-packaged applications offered in the software as a service model. But as the organization grows so to do the IT needs. Eventually, your company might grow so large you need to start developing your own applications based on the platforms and infrastructure models from Cloud providers. \n",
    "\n",
    "Why might you develop your own application? Well developing your own apps gives your IT team more control and flexibility over what the applications do but it also brings a new set of challenges. You'll need to figure out how the different pieces fit together, make sure that the services run reliably and troubleshoot problems when they come up. \n",
    "\n",
    "In the next few videos, we'll check out some of the options for building software for the Cloud. We'll look into the different types of storage available and how to decide which one to use. We'll also learn more about load balancing and how to distribute a service across many instances. We'll discuss how we can make changes to our systems without breaking everything. And we'll check out some of the limitations that you might run into when running software in the Cloud. We'll wrap up with some best practices for running reliable services. This includes measuring how your service is doing by using a monitoring system and also setting up alerts so you're automatically notified if things don't go as planned. Sometimes systems fail and that's okay. Using the tips you'll learn in this module you'll be prepared to deal with failure and troubleshoot issues when using Cloud services. Once we're done you'll get the chance to debug and fix a problem with an application running in the Cloud. Exciting stuff, right? There's a lot more ground to cover so let's dive in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Storing Data in the Cloud\n",
    "\n",
    "### 2.1 Choosing Your Cloud Provider \n",
    "\n",
    "Almost all IT systems need to store some data. Sometimes, it's a lot of data, sometimes, it's only bits and pieces of information. Cloud providers give us a lot of storage options. Picking the right solution for data storage will depend on what service you're building. You'll need to consider a bunch of factors, like\n",
    "\n",
    "- how much data you want to store\n",
    "\n",
    "- what kind of data that is\n",
    "\n",
    "- what geographical locations you'll be using it in \n",
    "\n",
    "- whether you're mostly writing or reading the data \n",
    "\n",
    "- how often the data changes\n",
    "\n",
    "- what your budget is\n",
    "\n",
    "This might sound like a lot of things to consider, but don't worry, it's not that bad. We'll check out some of the most common solutions offered by Cloud providers to give you a better idea of when to choose what. \n",
    "\n",
    "When choosing a storage solution in the Cloud, you might opt to go with the traditional storage technologies, like **block storage,** or you can choose newer technologies, like **object or blob storage.** Let's check out what each of these mean. \n",
    "\n",
    "### 2.2 Types of Storages\n",
    "\n",
    "#### 2.2.1 Block Storage\n",
    "\n",
    "As we saw in an earlier video, when we create a VM running in the Cloud, it has a local disk attached to it. These local disks are an example of block storage. This type of storage closely resembles the physical storage that you have on physical machines using physical hard drives. Block storage in the Cloud acts almost exactly like a hard drive. The operating system of the virtual machine will create and manage a file system on top of the block storage just as if it were a physical drive. There's a pretty cool difference though. These are virtual disks, so we can easily move the data around. For example, we can migrate the information on the disk to a different location, attach the same disk image to other machines, or create snapshots of the current state. All of this without having to ship a physical device from place to place. Our block storage can be either persistent or ephemeral. \n",
    "\n",
    "**Persistent storage** *is used for instances that are long lived, and need to keep data across reboots and upgrades.* On the flip side, **ephemeral storage** *is used for instances that are only temporary, and only need to keep local data while they're running.* Ephemeral storage is great for temporary files that your service needs to create while it's running, but you don't need to keep. This type of storage is especially common when using containers, but it can also be useful when dealing with virtual machines that only need to store data while they're running. \n",
    "\n",
    "In typical Cloud setups, each VM has one or more disks attached to the machine. The data on these disks is managed by the OS and can't be easily shared with other VMs. If you're looking to share data across instances, you might want to look into some **shared file system solutions,** that Cloud providers offer using the platform as a service model. When using these solutions, the data can be accessed through network file system protocols like NFS or CIFS. This lets you connect many different instances or containers to the same file system with no programming required. \n",
    "\n",
    "#### 2.2.2 Object / Blob Storage\n",
    "\n",
    "Block storage and shared file systems work fine when you're managing servers that need to access files. But if you're trying to deploy a Cloud app that needs to store application data, you'll probably need to look into other solutions like objects storage, which is also known as blob storage. **Object storage** *lets you place in retrieve objects in a storage bucket.* These objects are just generic files like photos or cat videos, encoded and stored on disk as binary data. These files are commonly called blobs, which comes from binary large object, and as we called out, these blobs are stored in locations known as buckets. \n",
    "\n",
    "![img1](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img1.jpg?raw=true)\n",
    "\n",
    "![img2](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img2.jpg?raw=true)\n",
    "\n",
    "\n",
    "### 2.3 Types of Databases\n",
    "\n",
    "Everything that you put into a storage bucket has a unique name. There's no file system. You place an object into storage with a name, and if you want that object back, you simply ask for it by name. To interact with an object store, you need to use an API or special utilities that can interact with the specific object store that you're using. On top of this, we've called out in earlier videos that most Cloud providers offer databases as a service. These come in two basic flavors, SQL and NoSQL. \n",
    "\n",
    "#### 2.3.1 SQL\n",
    "\n",
    "SQL databases, also known as relational, use the traditional database format and query language. Data is stored in tables with columns and rows that can be indexed, and we retrieve the data by writing SQL queries. A lot of existing applications already use this model, so it's typically chosen when migrating an existing application to the Cloud. \n",
    "\n",
    "#### 2.3.2 NoSQL\n",
    "\n",
    "NoSQL databases offer a lot of advantages related to scale. They're designed to be distributed across tons of machines and are super fast when retrieving results. But instead of a unified query language, we need to use a specific API provided by the database. This means that we might need to rewrite the portion of the application that accesses the DB.\n",
    "\n",
    "### 2.4 Storage Class\n",
    "\n",
    "#### 2.4.1 Variables\n",
    "\n",
    "When deciding how to store your data, you'll also have to choose a storage class. Cloud providers typically offer different classes of storage at different prices. Variables like performance, availability, or how often the data is accessed will affect the monthly price. The performance of a storage solution is influenced by a number of factors, including throughput, IOPS, and latency. Let's check out what these mean. \n",
    "\n",
    "**Throughput** *is the amount of data that you can read and write in a given amount of time.* The throughput for reading and writing can be pretty different. For example, you could have a throughput of one gigabyte per second for reading and 100 megabytes per second for writing. \n",
    "\n",
    "**IOPS or input/output operations** *per second measures how many reads or writes you can do in one second, no matter how much data you're accessing.* Each read or write operation has some overhead. So there's a limit on how many you can do in a given second, and \n",
    "\n",
    "**Latency** *is the amount of time it takes to complete a read or write operation.* This will take into account the impact of IOPS, throughput and the particulars of the specific service. Read latency is sometimes reported as the time it takes a storage system to start delivering data after a read request has been made, also known as **time to first byte.** While write latency is typically measured as the amount of time it takes for a write operation to complete. \n",
    "\n",
    "#### 2.4.2 Hot and Cold Data\n",
    "\n",
    "When choosing the storage class to use, you might come across terms like hot and cold. \n",
    "\n",
    "**Hot data** *is accessed frequently* and stored in hot storage while **cold data** *is accessed infrequently,* and stored in cold storage. These two storage types have different performance characteristics. For example, hot storage back ends are usually built using solid state disks, which are generally faster than the traditional spinning hard disks. So how do you choose between one and the other? Say you want to keep all the data you're service produces for five years, but you don't expect to regularly access data older than one year. You might choose to keep the last one year of data in hot storage so you have fast access to it, and after a year, you can move your data to cold storage where you can still get to it, but it will be slower and possibly costs more to access. \n",
    "\n",
    "---\n",
    "\n",
    "There's a lot more to say about storage in the Cloud. It's really quite a hot topic, but we won't go into more detail here. We'll provide links to more info in the next reading in case you want to learn more. Up next, we're going to look at a different Cloud scale characteristic that we've already touched on using multiple machines for the same service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Balancing\n",
    "\n",
    "In earlier videos, we saw a bunch of different reasons why we might want more than one machine or container running our service. For example, we might want to horizontally scale our service to handle more work, distribute instances geographically to get closer to our users. Or have backup instances to keep the service running if one or more of the instances fail. No matter the reason, we use orchestration tools and techniques to make sure that the instances are repeatable. And once we've set up replicated machines, we'll want to distribute the requests across instances. We called out earlier that this is where load balancing comes into play. \n",
    "\n",
    "### 3.1 Load Balancing Methods\n",
    "\n",
    "#### 3.1.1 Round Robin\n",
    "\n",
    "Let's take a closer look at the different load balancing methods that we can use. A pretty common load balancing technique is round robin DNS. Round robin is a really common method for distributing tasks. Imagine you're giving out treats at a party. First, you make sure that each of your friends gets one cookie. Then you give everyone a second serving and so on until all of the treats are gone or your guests say, thank you, they're full. That's the round-robin approach to eating all the cookies. Now, if we want to translate a URL like my service.example.com into an IP address, we use the DNS protocol or domain name system. In the simplest configuration, the URL always gets translated into exactly the same IP address. But when we configure our DNS to use round robin, it'll give each client asking for the translation a group of IP addresses in a different order. \n",
    "\n",
    "![img3](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img3.jpg?raw=true)\n",
    "\n",
    "The clients will then pick one of the addresses to try to reach the service. If an attempt fails, the client will jump to another address on the list.\n",
    "\n",
    "![img4](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img4.jpg?raw=true)\n",
    "\n",
    "This load balancing method is super easy to set up. You just need to make sure that the IPs of all machines in the pool are configured in your DNS server, but it has some limitations. First, you can't control which addresses get picked by the clients. Even if a server is overloaded, you can't stop the clients from reaching out to it. On top of that, DNS records are cached by the clients and other servers. So if you need to change the list of addresses for the instances, you'll have to wait until all of the DNS records that were cached by the clients expire. There's got to be a better way, right? Well, there sure is. To have more control over how the load's distributed and to make faster changes, we can set up a server as a dedicated load balancer. This is a machine that acts as a proxy between the clients and the servers. It receives the requests and based on the rules that we provide, it directs them to the selected back-end server.\n",
    "\n",
    "![img5](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img5.jpg?raw=true)\n",
    "\n",
    "#### 3.1.2 Sticky Sessons\n",
    "\n",
    "Load balances can be super simple or super complex depending on the service needs. Say your service needs to keep track of the actions that a user has taken up till now. In this case, you'll want your load balancer to use sticky sessions. Using **sticky sessions** *means all requests from the same client always go to the same back end server.* This can be really useful for services than need it but can also cause headaches when migrating or maintaining your service. So you need to use it only if you really need it. Otherwise, you'll end up in a really sticky situation. \n",
    "\n",
    "### 3.2 Health Checks with Load Balancers\n",
    "\n",
    "Another cool feature of load balancers is that you can configure them to check the health of the backend servers. Typically, we do this by making a simple query to the servers and checking that the reply matches the expected reply. If a back-end server is unhealthy, the load balancer will stop sending new requests to it to keep only healthy servers in the pool. \n",
    "\n",
    "![img6](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img6.jpg?raw=true)\n",
    "\n",
    "As we've called out a few times already, a cool feature of cloud infrastructure is how easily we can add or remove machines from a pool of servers providing a service. If we have a load balancer controlling the load of the machines, adding a new machine to the pool is as easy as creating the instance. And then letting the load balancer know that it can now route traffic to it. We can do this by manually creating and adding the instance or when our services under heavy load, we can just let the auto scaling feature do it. Cool, right? \n",
    "\n",
    "### 3.3 GeoDNS and GeoIP\n",
    "\n",
    "Okay, so imagine that you've built out your service with load balancers and you're receiving requests from all over the world. How do you make sure that clients connect to the servers that are closest to them? You can use GeoDNS and GeoIP. These are DNS configurations that will direct your clients to the closest geographical load balancer. The mechanism used to route the traffic relies on how the DNS servers respond to requests. For example, from machines hosted in North America, a DNS server in North America might be configured to respond with the IPs in, you guessed it, North America. It can be tricky to set this up on your own but most Cloud providers offer it as part of their services making it much easier to have a geographically distributed service. \n",
    "\n",
    "Let's take this one step further. There are some providers dedicated to bringing the contents of your services as close to the user as possible. These are the **content delivery networks or CDNs.** *They make up a network of physical hosts that are geographically located as close to the end user as possible.*\n",
    "\n",
    "This means that CDN servers are often in the same data center as the users Internet service provider. CDNs work by caching content super close to the user. When a user requests say, a cute cat video, it's stored in the closest CDN server. That way, when a second user in the same region requests the same cat video, it's already cached in a server that's pretty close and it can be downloaded extra fast. Because no one should have to wait for their cat videos to load.\n",
    "\n",
    "![img7](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img7.jpg?raw=true)\n",
    "\n",
    "![img8](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img8.jpg?raw=true)\n",
    "\n",
    "You now know a lot of stuff about load-balancing. Up next, we'll talk about another aspect of dealing with cloud services. How to deploy changes safely to our cloud service?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Change Management\n",
    "\n",
    "You've come a long way. You now know how to get your service running in the cloud. Next, let's talk about how to keep it running. Most of the time when something stops working, it's because something changed. If we want our cloud service to be stable, we might be tempted to avoid changes altogether. But change is a fact of cloud life. If we want to fix bugs and improve features in our services, we have to make changes. But we can make changes in a controlled and safe way. This is called **change management,** and it's what lets us keep innovating while our services keep running. Step one in improving the safety of our changes, we have to make sure they're well-tested. This means running unit tests and integration tests, and then running these tests whenever there's a change. \n",
    "\n",
    "### 4.1 Continuous Integration\n",
    "\n",
    "In an earlier course, we briefly mentioned continuous integration, or CI, but here's a refresher. A **continuous integration system** *will build and test our code every time there's a change.*\n",
    "\n",
    "Ideally, the CI system runs even for changes that are being reviewed. That way you can catch problems before they're merged into the main branch. You can use a common open source CI system like Jenkins, or if you use GitHub, you can use its Travis CI integration. Many cloud providers also offer continuous integration as a service. Once the change has committed, the CI system will build and test the resulting code. \n",
    "\n",
    "### 4.2 Continous Deployment\n",
    "\n",
    "Now you can use **continuous deployment,** or CD, to automatically deploy the results of the build or build artifacts. Continuous deployment lets you control the deployment with rules. For example, we usually configure our CD system to deploy new builds only when all of the tests have passed successfully.\n",
    "\n",
    "On top of that, we can configure our CD to push to different environments based on some rules. What do we mean by that? In an earlier video, we mentioned that when pushing puppet changes, we should have a test environment separate from the production environment.\n",
    "\n",
    "### 4.3 Environments and Pushing to Them\n",
    "\n",
    "Having them separate lets us validate that changes work correctly before they affect users. Here **environment** *means everything needed to run the service.* It includes the machines and networks used for running the service, the deployed code, the configuration management, the application configurations, and the customer data. Production, usually shortened to prod, is the real environment, the ones users see and interact with. Because of this, we have to protect, love, and nurture a prod. The test environment needs to be similar enough to prod that we can use them to check our changes work correctly. You could have your CD system configured to push new changes to the test environment. You can then check that the service is still working correctly there, and then manually tell your deployment system to push those same changes to production.\n",
    "\n",
    "![img9](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img9.jpg?raw=true)\n",
    "\n",
    "If the service is complex and there are a bunch of different developers making changes to it, you might set up additional environments where the developers can test their changes in different stages before releasing them. For example, you might have your CD system push all new changes to a development or dev environment, then have a separate environment called pre-prod, which only gets specific changes after approval. And only after a thorough testing, these changes get pushed to prod. \n",
    "\n",
    "Say you're trying to increase the efficiency of your surface by 20%, but you don't know if the change you made might crash part of your system. You want to deploy it to one of those testing or development environments to make sure it works correctly before you ship it to prod. Remember, these environments need to be as similar to prod as possible. They should be built and deployed in the same way. And while we don't want them to be breaking all the time, it's normal for some changes to break dev or even pre-prod. We're just happy that we can catch them early so that they don't break prod. \n",
    "\n",
    "### 4.4 A/B Testing\n",
    "\n",
    "Sometimes you might want to experiment with a new service feature. You've tested the code, you know it works, but you want to know if it's something that's going to work well for your users. When you have something that you want to test in production with real customers, you can experiment using A/B testing. In **A/B testing,** *some requests are served using one set of code and configuration, A, and other requests are served using a different set of of code and configuration, B.* This is another place where a load balancer and instance groups can help us out. \n",
    "\n",
    "![img10](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img10.jpg?raw=true)\n",
    "\n",
    "You can deploy one instance group in your A configuration and a second instance group in your B configuration. Then by changing the configuration of the load balancer, you can direct different percentages of inbound requests to those two configurations.\n",
    "\n",
    "![img11](https://github.com/Brian-E-Nguyen/Google-IT-Automation-with-Python/blob/5-Config-Management-and-Cloud/5-Config-Management-and-Cloud/Week-4-Managing-Cloud-Instances-at-Scale/img/img11.jpg?raw=true)\n",
    "\n",
    "If your A configuration is today's production configuration and your B configuration is something experimental, you might want to start by only directing 1 % of your requests to B. Then you can slowly ramp up the percentage that you check out whether the B configuration performs better than A, or not. \n",
    "\n",
    "Heads up, make sure you have basic monitoring so that it's easy to tell if A or B is performing better or worse. If it's hard to identify the back-end responsible for serving A requests or B requests, then much of the value of A/B testing is lost to A/B debugging. So what happens if all the precautions we took aren't enough and we break something in production? Remember what we discussed in an earlier course about post-mortems. We learn from fail\n",
    "\n",
    "Ask yourself, what did I have to do to catch the problem? Can I have one of my change management systems look for problems like that in the future? Can I add a test or a rule to my unit tests, my CI/CD system, or my service health checks to prevent this kind of failure in the future? So remember, if something breaks, give yourself a break. Sometimes in IT, these things happen, no matter how careful you are. And as you use and refine your change management systems and skills, you'll gain the confidence to make changes to your service more quickly and safely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
